# AI-Extension Application Hub - User Manual 

## Purpose
The AI-Extensions project aims at supporting the Earth Science and Services Communities by expanding the existing Earth Observation (EO) platform offerings services with operationally mature AI/ML software capabilities. This is achieved through the AI-Extension Application Hub, a dedicated Cloud platform that provides the integration and operational implementation of EO and AI capabilities.  

This User Manual provides a guideline, as well as step-by-step instructions, for developers and consumers to effectively leverage the AI-Extension ML-Lab. 

## Core Services
The Application Hub ML-Lab service leverages **JupyterHub** as an Application Hub to manage and deploy various web applications, including **MLflow**, **JupyterLab**, **Code Server**, **QGIS Remote Desktop**, and **STAC Browser**. **JupyterHub** acts as a central platform, facilitating the launching and management of these applications, providing a seamless and integrated experience for our users.

* **JupyterHub**: JupyterHub is a dedicated multi-user server that brings the power of JupyterLab to collaborative environments. It allows organizations to effortlessly deploy and manage Jupyter Notebook servers for multiple users, enabling seamless collaboration and resource sharing in data science and research settings. 
* **MLflow**: MLflow is an open-source platform for managing the ML lifecycle. MLflow provides tools and functionalities to track experiments, package models, version models, model hyperparameters, packaging code into reproducible runs, deploy models, monitor performance, and enable collaboration among team members. MLflow enables users to effectively organize and monitor their ML projects, enabling collaboration, reproducibility, and streamlined deployment workflows.
* **JupyterLab**: JupyterLab is a powerful and flexible web-based Integrated Development Environment (IDE) for data analysis, model development, and interactive scientific computing. With JupyterLab, our users can write and execute code, visualise data, and create rich, interactive notebooks, enabling efficient experimentation and exploration of their data. With its flexible and extensible architecture, JupyterLab provides a seamless interface for data science workflows, allowing AI-users to explore, analyze, and collaborate on data-driven projects effortlessly. 
* **Code Server**: Code Server enables users to run Visual Studio Code (VS Code), a lightweight and versatile source code editor that combines the simplicity of a text editor with powerful developer tools, providing an intuitive and customizable environment for coding across various programming languages. With Code Server, VS Code and all its functionalities are available directly from the Application Hub server. 
* **QGIS Remote Desktop**: a remote desktop environment with QGIS, a widely-used free and open-source application for viewing, editing, and analysing geo-spatial data. It provides a versatile platform equipped with tools to perform spatial analysis, geoprocessing, visualise geospatial data, enhancing their labelling and analysis tasks. This integration empowers AI-users to leverage QGIS's extensive capabilities directly from their web browser for making informed decisions based on geographic data.
* **STAC Browser**: The SpatioTemporal Asset Catalog (STAC) is a powerful standard for describing geospatial data, so it can be more easily worked with, indexed, discovered and shared. As part of the Application Hub, the system integrates the STAC Browser, which allows AI-users to discover and explore EO data and training datasets. The STAC Browser supports easy search and retrieval of relevant EO data through the STAC standard, enabling efficient data discovery and selection for ML tasks.

## User Scenarios
* User Scenario 1 - Alice does Exploratory Data Analysis (EDA): The purpose of Explaratory Data Analysis (EDA) is to analyse the data that will be used to train and evaluate Machine Learning models. In this Notebook are firstly shown the steps to access and visualize EO data (e.g. Sentinel-2 scenes) and their metadata using STAC API. Secondly, a pre-arranged DataFrame containing labeled geospatial data with reflectance values and vegetation indices is loaded and used for the purpose of EDA.
* User Scenario 2 - Alice labels Earth Observation data: Labelling data is a crucial step in the process for developing supervised Machine Learning (ML) models. It involves the critical task of assigning relevant labels or categories to different features within the data, such as land cover class (e.g. vegetation, water bodies, urban area, etc.) or other physical characteristics of the Earth's surface. These labels can be binary (e.g., water or non-water) or multi-class (e.g., forest, grassland, urban).
* User Scenario 3 - Alice describes the labelled Earth Observation data: Labeled Earth Observation (EO) data can be described using the SpatioTemporal Asset Catalog (STAC) standard. This allows to describe the labeled EO data while defining standardized sets of metadata to delineate its key properties, such as spatial and temporal extents, resolution, and other pertinent characteristics. Additionally, it enables the user to include details about the labeling process itself, as well as enabling specific parameters-driven queries on the catalog.
* User Scenario 4 - Alice discovers labelled Earth Observation data: This Scenario documents the process for discovering labelled EO data described with the STAC standard. One of the notable advantages of the STAC-supported catalogs is the ability to filter search results using STAC metadata. This empowers the user to narrow down the search based on specific labeling criteria. By applying such filters, the user can pinpoint the datasets that align with the specific requirements.
* User Scenario 5 - Alice develops a new Machine Learning model: During the implementation of this Scenario, two Notebooks were developed:
  * **“Alice develops a new machine learning”** Notebook: this is the main Notebook for Scenario 5, as it follows the requirements for Scenario 5 described in the Service Specification document. 
  * **“Implementation of EuroSAT STAC dataset”** Notebook: the objective of this Notebook was to generate STAC Catalog, Collection and Items of the EuroSAT dataset. The EuroSAT STAC dataset was then used as input dataset for developing and testing the main **“Alice develops a new machine learning”** Notebook. 
* User Scenario 6 - Alice starts a training job on a remote
* User Scenario 7 ​- Alice describes her trained machine learning model
* User Scenario 8 -​ Alice reuses an existing pre-trained model
* User Scenario 9 - Alice creates a training dataset
* User Scenario 10 - Eric discovers a model and consumes it

## User Showcases

