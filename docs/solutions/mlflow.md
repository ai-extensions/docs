# ML-Flow
`MLflow` is a powerful open-source platform that simplifies the end-to-end machine learning (ML) lifecycle management. It provides tools for tracking experiments, model hyperparameters, packaging code into reproducible runs, and sharing and deploying models across different environments seamlessly. `MLflow` enables users to effectively organize and monitor their ML projects, enabling collaboration, reproducibility, and streamlined deployment workflows. Link to the official documentation: https://mlflow.org/.

On the JupyterLab dashboard, click on the `mlflow` Logo.

<p align="left" ><img src="../../images/mlflow_icon.png" alt="Picture" width="100" height="100"style=" display: block; margin: 20 auto;"/></p>

The `MLflow` dashboard will appear.

<p align="left" ><img src="../../images/mlflow_dashboard.png" alt="Picture" width="80%"style=" display: block; margin: 20 auto;"/></p>

Below are a few examples of using `MLflow` in a ML project workflow:
* The user can select one or multiple runs to **Compare**

<p align="left" ><img src="../../images/selectrun.png" alt="Picture" width="60%" height="100%"style=" display: >block; margin: 20 auto;"/></p>

* The user can see a quick overview of each run and select which parameter(s) to analyse and plot on the graph

<p align="left" ><img src="../../images/rundetails.png" alt="Picture" width="60%" height="100%" style=" display: >block; margin: 20 auto;"/></p>
       
* The user compares different parameteres fed to the CNN model

<p align="left" ><img src="../../images/parameters.png" alt="Picture" width="60%" height="100%"style=" display: >block; margin: 20 auto;"/></p> 

* The user compares evaluation metrics of each run, to opt for the best model for his/her application. 

<p align="left" ><img src="../../images/metrics.png" alt="Picture" width="60%" height="100%"style=" >display: >block; margin: 20 auto;"/></p>        
