{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"AI-Extension Application Hub - User Manual","text":""},{"location":"#purpose","title":"Purpose","text":"<p>The AI-Extensions project aims at supporting the Earth Science and Services Communities by expanding the existing Earth Observation (EO) platform offerings services with operationally mature AI/ML software capabilities. This is achieved through the AI-Extension Application Hub, a dedicated Cloud platform that provides the integration and operational implementation of EO and AI capabilities.  </p> <p>This User Manual provides a guideline, as well as step-by-step instructions, for developers and consumers to effectively leverage the AI-Extension ML-Lab. </p>"},{"location":"#core-services","title":"Core Services","text":"<p>The Application Hub ML-Lab service leverages JupyterHub as an Application Hub to manage and deploy various web applications, including MLflow, JupyterLab, Code Server, QGIS Remote Desktop, and STAC Browser. JupyterHub acts as a central platform, facilitating the launching and management of these applications, providing a seamless and integrated experience for our users.</p> <ul> <li>JupyterHub: JupyterHub is a dedicated multi-user server that brings the power of JupyterLab to collaborative environments. It allows organizations to effortlessly deploy and manage Jupyter Notebook servers for multiple users, enabling seamless collaboration and resource sharing in data science and research settings. </li> <li>MLflow: MLflow is an open-source platform for managing the ML lifecycle. MLflow provides tools and functionalities to track experiments, package models, version models, model hyperparameters, packaging code into reproducible runs, deploy models, monitor performance, and enable collaboration among team members. MLflow enables users to effectively organize and monitor their ML projects, enabling collaboration, reproducibility, and streamlined deployment workflows.</li> <li>JupyterLab: JupyterLab is a powerful and flexible web-based Integrated Development Environment (IDE) for data analysis, model development, and interactive scientific computing. With JupyterLab, our users can write and execute code, visualise data, and create rich, interactive notebooks, enabling efficient experimentation and exploration of their data. With its flexible and extensible architecture, JupyterLab provides a seamless interface for data science workflows, allowing AI-users to explore, analyze, and collaborate on data-driven projects effortlessly. </li> <li>Code Server: Code Server enables users to run Visual Studio Code (VS Code), a lightweight and versatile source code editor that combines the simplicity of a text editor with powerful developer tools, providing an intuitive and customizable environment for coding across various programming languages. With Code Server, VS Code and all its functionalities are available directly from the Application Hub server. </li> <li>QGIS Remote Desktop: a remote desktop environment with QGIS, a widely-used free and open-source application for viewing, editing, and analysing geo-spatial data. It provides a versatile platform equipped with tools to perform spatial analysis, geoprocessing, visualise geospatial data, enhancing their labelling and analysis tasks. This integration empowers AI-users to leverage QGIS's extensive capabilities directly from their web browser for making informed decisions based on geographic data.</li> <li>STAC Browser: The SpatioTemporal Asset Catalog (STAC) is a powerful standard for describing geospatial data, so it can be more easily worked with, indexed, discovered and shared. As part of the Application Hub, the system integrates the STAC Browser, which allows AI-users to discover and explore EO data and training datasets. The STAC Browser supports easy search and retrieval of relevant EO data through the STAC standard, enabling efficient data discovery and selection for ML tasks.</li> </ul>"},{"location":"#user-scenarios","title":"User Scenarios","text":"<ul> <li>User Scenario 1 - Alice does Exploratory Data Analysis (EDA): The purpose of Explaratory Data Analysis (EDA) is to analyse the data that will be used to train and evaluate Machine Learning models. In this Notebook are firstly shown the steps to access and visualize EO data (e.g. Sentinel-2 scenes) and their metadata using STAC API. Secondly, a pre-arranged DataFrame containing labeled geospatial data with reflectance values and vegetation indices is loaded and used for the purpose of EDA.</li> <li>User Scenario 2 - Alice labels Earth Observation data: Labelling data is a crucial step in the process for developing supervised Machine Learning (ML) models. It involves the critical task of assigning relevant labels or categories to different features within the data, such as land cover class (e.g. vegetation, water bodies, urban area, etc.) or other physical characteristics of the Earth's surface. These labels can be binary (e.g., water or non-water) or multi-class (e.g., forest, grassland, urban).</li> <li>User Scenario 3 - Alice describes the labelled Earth Observation data: Labeled Earth Observation (EO) data can be described using the SpatioTemporal Asset Catalog (STAC) standard. This allows to describe the labeled EO data while defining standardized sets of metadata to delineate its key properties, such as spatial and temporal extents, resolution, and other pertinent characteristics. Additionally, it enables the user to include details about the labeling process itself, as well as enabling specific parameters-driven queries on the catalog.</li> <li>User Scenario 4 - Alice discovers labelled Earth Observation data: This Scenario documents the process for discovering labelled EO data described with the STAC standard. One of the notable advantages of the STAC-supported catalogs is the ability to filter search results using STAC metadata. This empowers the user to narrow down the search based on specific labeling criteria. By applying such filters, the user can pinpoint the datasets that align with the specific requirements.</li> <li>User Scenario 5 - Alice develops a new Machine Learning model: During the implementation of this Scenario, two Notebooks were developed:<ul> <li>\u201cAlice develops a new machine learning\u201d Notebook: this is the main Notebook for Scenario 5, as it follows the requirements for Scenario 5 described in the Service Specification document. </li> <li>\u201cImplementation of EuroSAT STAC dataset\u201d Notebook: the objective of this Notebook was to generate STAC Catalog, Collection and Items of the EuroSAT dataset. The EuroSAT STAC dataset was then used as input dataset for developing and testing the main \u201cAlice develops a new machine learning\u201d Notebook. </li> </ul> </li> <li>User Scenario 6 - Alice starts a training job on a remote: This scenario involves executing a training job remotely on a different machine, which advantages are twofold: one one hand, the remote machine can be provisioned on-demand through a cloud provider, providing the user with the flexibility to access additional resources such as CPUs and GPUs; on the other hand, this enables the user to keep on working without being hindered by the resource-intensive and time-consuming training process. Once the experiment begins on the remote machine, the user receives a notification containing a link to the experiment in MLflow, enabling an active monitoring on the training progress while reviewing the results in real-time. </li> <li>User Scenario 7 \u200b- Alice describes her trained machine learning model: This Scenario leverabes the capabilities of STAC to provide a comprehensive and standardised description of a trained ML model. The user will create a STAC Item file that encapsulates the relevant metadata, such as: model name and version, description of the model architecture and training process, specifications of input and output data formats, performance metrics and evaluation results, details regarding model deployment and usage. This approach not only facilitates collaboration but also promotes interoperability within the geospatial and ML communities.</li> <li>User Scenario 8 -\u200b Alice reuses an existing pre-trained model: In this Scenario, Alice leverages the power of transfer learning (a widely adopted technique in deep learning that involves employing a pre-trained model on a large dataset (such as ImageNet) as a starting point to train a new ML model on a smaller dataset) by utilising a pre-trained ML model as a foundation for her ML endeavours. This allows the new ML model to achieve higher accuracy even when trained with limited data.</li> <li>User Scenario 9 - Alice creates a training dataset: In this Scenario, Alice creates a deep learning dataset with image chips, which involves extracting smaller image patches (chips) from a large EO image and associating them with corresponding land-cover labels, using a semantic segmentation approach. </li> <li>User Scenario 10 - Eric discovers a model and consumes it: In this Scenario, a geospatial data analyst such as Eric discovers an ML model, created by other practitioners such as Alice, by using the STAC catalog\u2019s search functionalities. This enables Eric to use relevant keywords, such as the model name, geographic location, or application domain, to narrow down the search. The data catalog's search interface allows Eric to explore the available ML models that match the search criteria and are described using STAC. Once the ML model is found, it can be run on other geospatial data, integrating it into a larger workflow, or applying it within a specific application context. The ML model will then be deployed as a processing service on an exploitation platform using the OGC API processes. By following these steps, Eric can seamlessly integrate Alice's model into his existing infrastructure and leverage its capabilities within his geospatial analysis workflows.</li> </ul>"},{"location":"#user-showcases","title":"User Showcases","text":"<ul> <li>Urban greenery: This showcase focuses on the development and application of AI approaches for urban greenery using EO data. The objective is to support the assessment and implementation of Natural-Based Solutions (NBS) to address urban challenges, specifically focusing on monitoring urban heat patterns and preventing flooding in urban areas. </li> <li>Informal settlement: This showcase focuses on the development and application of AI approaches for EO data in the context of urban management, specifically targeting the challenges posed by informal settlements. The objective is to provide objective geospatial data and analytics to support urban management and address the needs of customers in assessing and managing informal settlements.</li> <li>Geohazards - volcanoes: The Geohazards volcanoes showcase focus on the development and application of AI approaches for EO data in the context of monitoring and assessing volcanic hazards. The proposed activities aim to address the need for a dynamical monitoring tool combining ML and Deep Learning (DL) algorithms with SAR data from the Sentinel-1 satellite. These activities will be conducted in the GEP to leverage its capabilities and ensure efficient model development, training, and deployment. The activities can be divided into several key phases:</li> </ul>"},{"location":"1-intro/1-foreword/","title":"Foreword","text":"<p>The AI-Extensions project aims at supporting the Earth Science and Services Communities by expanding the existing Earth Observation (EO) platform offerings services with operationally mature AI/ML software capabilities. This is achieved through the AI-Extension Application Hub (ML-Lab), a dedicated Cloud platform that provides the integration and operational implementation of EO and AI capabilities.  </p> <p>This User Manual provides a guideline, as well as step-by-step instructions, for developers and consumers to effectively leverage the AI-Extension Application Hub ML-Lab. </p>"},{"location":"1-intro/2-registration/","title":"Registration","text":"<p>Note: while being under development, the registration to the ML-Lab is for early-adopters users only, working and testing dedicated Showcases.</p>"},{"location":"1-intro/2-registration/#create-a-user-account","title":"Create a user account","text":"<ol> <li>Create an account and register on Terradue portal  your account https://www.terradue.com/portal/signup. Please note not to use special characters (including -, ., _, etc) for the username, but only just letters and numbers. </li> <li>Check your mailbox. We have sent you a confirmation email in order to validate your user email address. Click on the link to complete registration process.</li> </ol>"},{"location":"1-intro/2-registration/#notify-registered-account-to-terradue","title":"Notify registered account to Terradue","text":"<ol> <li>Notify Terradue (support@terradue.com) about successful registration, by providing your registered username.</li> </ol>"},{"location":"1-intro/2-registration/#dedicated-ml-lab-instance","title":"Dedicated ML-Lab instance","text":"<ol> <li>A dedicated ML-Lab instance will be created to your provided username</li> <li>You can access your ML-Lab via the provided URL (e.g. https://app-hub-ai-extensions-dev.terradue.com/</li> </ol>"},{"location":"2-core/1-intro/","title":"Application Hub ML-Lab","text":"<p>The Application Hub is a comprehensive and modular platform delivering Software-as-a-Service (SaaS) products, designed to cater to the diverse and multifaceted needs of the EO community. It is crafted to support a wide array of stakeholders, from developers and service providers integrating cutting-edge algorithms to researchers harnessing computational power, and analysts requiring clear and concise visualisations. At the heart of the Application Hub is the ability to manage the delivery of work environments and tools for a wide range of user tasks, such as develop, host, execute, and perform exploratory analysis of EO applications, all managed within a single, unified Cloud infrastructure.</p> <p>The Application Hub, leveraging Kubernetes and JupyterHub, creates a robust, scalable, and user-centric platform for EO applications and analytics. Kubernetes ensures scalable operation of containerized applications by managing deployment, operation, and traffic distribution, while JupyterHub orchestrates the launching, scaling, and management of application instances, acting as the primary gateway for user requests. The Hub uses dedicated namespaces for each application pod, ensuring organisation, security, and isolation. It also dynamically configures application pods based on the task, and personalises the experience based on user profiles through Kube Spawner. This design ensures the Application Hub remains modular, scalable, and capable of catering to the dynamic requirements of EO tasks.</p> <p>Typically, the Application Hub provides access to platforms and web apps in a SaaS mode. Users can engage with containerized Interactive Graphical Applications (IGAs), specialised geospatial data exploration web apps, and customizable dashboards. This allows users not only to explore and analyse results but also to execute new applications or analyses and customise their computing experiences, all accessed from the same integrated Hub interface. Ultimately, this enhances user experience, optimises software usage costs, and promotes ease of use, making it more accessible to the broader EO community.</p> <p>The Application Hub ML-Lab service leverages JupyterHub as an Application Hub to manage and deploy various web applications, including MLflow, JupyterLab, Code Server, QGIS Remote Desktop, and STAC Browser. JupyterHub acts as a central platform, facilitating the launching and management of these applications, providing a seamless and integrated experience for our users.</p> <p>By harnessing the Application Hub\u2019s capabilities, our service creates a unified and efficient environment where users can seamlessly switch between these web applications based on their specific needs. This integrated approach enhances productivity, collaboration, and the overall user experience, enabling our users to effectively utilise these applications to accomplish their goals.</p>"},{"location":"2-core/2-jupyterhub/","title":"JupyterHub","text":"<p>JupyterHub is a dedicated multi-user server that brings the power of JupyterLab to collaborative environments. It allows organizations to effortlessly deploy and manage Jupyter Notebook servers for multiple users, enabling seamless collaboration and resource sharing in data science and research settings. </p> <p>After login on your dedicated App Hub instance (e.g. https://app-hub-ai-extensions-dev.terradue.com/), you will be given the choice between different server options. These options are user-specific and depend on your registration profile settings. </p> <p>In the example shown below, seven server options are available: </p> <p></p> <p>JupyterHub acts as a central platform, facilitating the launching and management of applications and providing a seamless and integrated experience for our users. Various web applications such as MLflow, JupyterLab, Code Server are managed and deployed within JupyterHub. </p> <p>JupyterHub can be launched by selecting either Small or Large servers, according to the resources needed by the user: </p> <ul> <li>Machine Learning Lab - Small: server with 1 CPU and 4GB of RAM </li> <li>Machine Learning Lab - Large: server with 3 CPU and 14GB of RAM</li> </ul> <p>Other applications can be launched by selecting their dedicated instances and then clicking on <code>Start</code>.</p> <ul> <li>Machine Learning Lab with GPU 0.10</li> <li>STAC Browser Earth-Search AWS</li> <li>STAC Browser for AI-Extensions STAC API</li> <li>QGIS (includes tooling and plugins v0.4 aws)</li> </ul>"},{"location":"2-core/3-mlflow/","title":"MLflow","text":""},{"location":"2-core/3-mlflow/#introduction","title":"Introduction","text":"<p>The ML-Lab application integrates with MLflow, an open-source platform for managing the machine learning (ML) lifecycle. MLflow provides tools and functionalities to track experiments, package models, version models, model hyperparameters, packaging code into reproducible runs, deploy models, monitor performance, and enable collaboration among team members. MLflow enables users to effectively organize and monitor their ML projects, enabling collaboration, reproducibility, and streamlined deployment workflows.</p> <p>The following outlines the specific ways in which MLflow is used within the service:</p> <ul> <li>Experiment Tracking: The service utilises MLflow's experiment tracking capabilities to record parameters, metrics, and artefacts associated with each machine learning experiment. This enables reproducibility and facilitates comparison between different model configurations.</li> <li>Model Packaging: MLflow is used to package trained models in a standardised format. This includes saving the model artefacts, dependencies, and metadata required for deployment. The packaged models are self-contained and can be easily shared or deployed in various environments.</li> <li>Model Versioning: MLflow's versioning functionality is leveraged to manage different versions of trained models. This allows for tracking the evolution of models over time, comparing different versions, and ensuring reproducibility. It also facilitates collaboration among team members working on the same project.</li> <li>Integration with Existing Tools: MLflow integrates with popular machine learning tools and frameworks such as <code>TensorFlow</code>, <code>PyTorch</code>, and <code>scikit-learn</code>. This allows seamless integration of MLflow's tracking and management capabilities within the existing machine learning workflow, enhancing productivity and compatibility.</li> </ul> <p>By incorporating MLflow into the ML-Lab application, we ensure proper management, tracking, packaging, versioning, deployment, monitoring, and collaboration throughout the machine learning lifecycle. This enables efficient development, deployment, and maintenance of machine learning models within the service.</p>"},{"location":"2-core/3-mlflow/#starting-mlflow","title":"Starting MLflow","text":"<p>On the JupyterLab dashboard, click on the <code>mlflow</code> Logo.</p> <p></p> <p>The <code>MLflow</code> dashboard will appear.</p> <p></p>"},{"location":"2-core/3-mlflow/#practical-examples","title":"Practical Examples","text":"<p>Below are a few examples of using <code>MLflow</code> in a ML project workflow:</p> <ul> <li>The user can select one or multiple runs to Compare</li> </ul> <p></p> <ul> <li>The user can see a quick overview of each run and select which parameter(s) to analyse and plot on the graph</li> </ul> <p></p> <ul> <li>The user compares different parameteres fed to the CNN model</li> </ul> <p></p> <ul> <li>The user compares evaluation metrics of each run, to opt for the best model for his/her application. </li> </ul> <p></p>"},{"location":"2-core/4-jupyterlab/","title":"JupyterLab","text":""},{"location":"2-core/4-jupyterlab/#introduction","title":"Introduction","text":"<p>JupyterLab, serves as a powerful and flexible web-based Integrated Development Environment (IDE) for data analysis, model development, and interactive scientific computing. With JupyterLab, our users can write and execute code, visualise data, and create rich, interactive notebooks, enabling efficient experimentation and exploration of their data. With its flexible and extensible architecture, JupyterLab provides a seamless interface for data science workflows, allowing AI-users to explore, analyze, and collaborate on data-driven projects effortlessly. </p> <p>JupyterLab provides a flexible and powerful environment that supports AI-users in the implementation of the User Scenarios, enabling efficient exploration, prototyping, and development of ML models. The following outlines the specific ways in which JupyterLab is used within the service:</p> <ul> <li>Interactive Data Analysis: JupyterLab enables AI-users to perform interactive data analysis using Python and other programming languages. They can write and execute code in Jupyter notebooks, visualise data, and generate insights. This allows her to explore and understand the training data, perform statistical analysis, and gain valuable insights into the underlying patterns and trends.</li> <li>Model Development and Training: AI-users can leverage JupyterLab's capabilities to build and train ML models. They can use popular ML libraries such as <code>TensorFlow</code>, <code>PyTorch</code>, or <code>scikit-learn</code> to develop and experiment with different models. JupyterLab's interactive nature allows for iterative model development, parameter tuning, and real-time monitoring of training progress.</li> <li>Data Visualization: JupyterLab provides powerful data visualisation tools, allowing AI-users to create rich and interactive visualisations of her data and model outputs. They can generate plots, charts, and graphs to analyse model performance, understand feature importance, and communicate results effectively.</li> <li>Code Reusability and Collaboration: JupyterLab supports code reusability and collaboration among team members. AI-users can organise her code into reusable functions or modules, making it easier to maintain and share across different projects. JupyterLab also facilitates collaborative work, allowing multiple team members to work on the same notebooks simultaneously, comment on code, and provide feedback.</li> <li>Documentation and Reporting: JupyterLab's notebook format provides a powerful means of documenting and reporting ML experiments. AI-users can combine code, visualisations, and narrative text in a single document, making it easier to communicate and share her work with stakeholders. Notebooks can be exported in various formats, such as HTML or PDF, for easy dissemination.</li> <li>Integration with Data Science Libraries: JupyterLab seamlessly integrates with a wide range of data science libraries and frameworks, providing access to a rich ecosystem of tools and resources. AI-users can leverage these libraries to perform tasks such as data preprocessing, feature engineering, model evaluation, and more, enhancing her productivity and enabling faster development cycles.</li> </ul> <p>By incorporating JupyterLab into the service, we empower AI-users with a versatile and user-friendly environment for data analysis, model development, visualisation, and collaboration. JupyterLab's interactive and flexible nature aligns perfectly with AI-users's scenarios, allowing her to leverage its capabilities to achieve her goals effectively.</p>"},{"location":"2-core/4-jupyterlab/#starting-jupyterlab","title":"Starting JupyterLab","text":"<p>After loading up, the JupyterLab dashboard will appear. </p> <p></p>"},{"location":"2-core/5-codeserver/","title":"Code Server","text":""},{"location":"2-core/5-codeserver/#introduction","title":"Introduction","text":"<p>Code Server enables users to run Visual Studio Code (VS Code), a lightweight and versatile source code editor that combines the simplicity of a text editor with powerful developer tools, providing an intuitive and customizable environment for coding across various programming languages. With Code Server, VS Code and all its functionalities are available directly from the Application Hub server. </p> <p>Code Server extends the capabilities of JupyterLab by providing a cloud-based IDE with GPU resources that enables efficient coding, collaboration, and development of ML models. It enables AI-users to build, train, and test ML models using popular frameworks like <code>PyTorch</code> or <code>TensorFlow</code>. The GPU support enhances model training performance, allowing for faster iterations and improved productivity.</p> <p>The following outlines the specific ways in which Code Server is used within the service:</p> <ul> <li>Development Environment: Code Server provides a cloud-based development environment that eliminates the need for local installations and configurations. AI-users can access Code Server through a web browser, enabling her to work from any device with an internet connection. This flexibility allows her to work remotely, collaborate with team members, and access her projects and codebase from anywhere.</li> <li>Code Editing and Collaboration: Code Server offers a powerful code editing experience with features like syntax highlighting, code completion, and code navigation. AI-users can write, edit, and debug code in various programming languages, including Python, R, or Julia. Code Server also supports collaboration features, allowing multiple users to edit and work on the same codebase simultaneously, enabling efficient teamwork and knowledge sharing.</li> <li>ML Model Development: AI-users can leverage Code Server to develop ML models efficiently. They can use popular ML frameworks and libraries like <code>TensorFlow</code>, <code>PyTorch</code>, or <code>scikit-learn</code> to build, train, and evaluate models. Code Server's integrated terminal allows her to execute commands and run experiments seamlessly.</li> <li>Git Integration: Code Server integrates with Git, a version control system, enabling AI-users to manage her codebase effectively. They can perform Git operations such as cloning repositories, creating branches, committing changes, and pushing code to remote repositories directly from within Code Server. This integration facilitates code versioning, collaboration, and easy synchronisation with code hosting platforms like GitHub or GitLab.</li> <li>Extension Ecosystem: Code Server provides an extension ecosystem that allows AI-users to customise and enhance her development environment. They can install extensions for additional functionalities, such as linting, code formatting, code snippets, or ML-specific tools. This extensibility ensures that AI-users can tailor her coding environment to meet her specific needs and preferences.</li> <li>Integrated Terminal: Code Server includes an integrated terminal, enabling AI-users to execute command-line operations without leaving the IDE. They can run scripts, install dependencies, manage virtual environments, and perform various system-level tasks directly within Code Server. This seamless integration streamlines her workflow and eliminates the need for switching between different tools.</li> </ul> <p>By incorporating Code Server into the service, we provide AI-users with a cloud-based IDE that supports her scenarios and enables efficient coding, collaboration, and ML model development. Code Server's accessibility, collaboration features, and integration with essential tools like Git make it an ideal choice for empowering AI-users to achieve her goals effectively.</p>"},{"location":"2-core/5-codeserver/#starting-code-server","title":"Starting Code Server","text":"<p>On the JupyterLab dashboard, click on the Code Server Logo.</p> <p></p> <p>The Code Server dashboard will appear.</p> <p></p> <p>You have access of all these functionalities from the vertical panel in the top-left corner of the dashboard:</p> <ul> <li>Menu: access functions and settings within VS Code</li> <li>Explore: navigate and manage files and directories in your workspace </li> <li>Search: find specific files, text, or symbols within your workspace</li> <li>Source Control: manage version control system such as Git directly within VS Code</li> <li>Run and Debug: execute and debug code with built-in tools</li> <li>Extensions: enhance functionality by installing and managing extensions to support the development workflow </li> <li>Test: run tests and view test outptus.</li> </ul>"},{"location":"2-core/6-aws/","title":"Access to Object Storage","text":""},{"location":"2-core/6-aws/#introduction","title":"Introduction","text":"<p>A dedicated object storage (in this example Amazon S3) is pre-configured to be accessed from the App Hub. This can be done with the Amazon Web Server (AWS) <code>aws s3</code> commands in the AWS CLI.</p>"},{"location":"2-core/6-aws/#practical-example","title":"Practical Example","text":"<p>For example, to list the content of a specific S3 bucket, you can use the command below. <pre><code>aws s3 ls &lt;bucket_name&gt;\n</code></pre> Other examples with full syntax on using the <code>aws s3</code> command are described in the official AWS website.</p>"},{"location":"3-plus/1-qgis/","title":"QGIS","text":""},{"location":"3-plus/1-qgis/#introduction","title":"Introduction","text":"<p>Our service incorporates QGIS Remote Desktop, managed by JupyterHub, providing a remote desktop environment with QGIS, a widely-used free and open-source application for viewing, editing, and analysing geo-spatial data. It provides a versatile platform equipped with tools to perform spatial analysis, geoprocessing, visualise geospatial data, enhancing their labelling and analysis tasks. This integration empowers AI-users to leverage QGIS's extensive capabilities directly from their web browser for making informed decisions based on geographic data.</p> <p>The following outlines the specific ways in which QGIS supports AI-users's labelling tasks within the service:</p> <ul> <li>Geospatial Data Exploration: QGIS allows AI-users to load and explore various types of geospatial data within the remote desktop environment. They can import satellite imagery, aerial photographs, vector data, and other geospatial datasets. QGIS provides a user-friendly interface to navigate and zoom in on specific areas of interest, enabling AI-users to closely examine the data they need to label.</li> <li>Labelling Tools and Annotations: QGIS offers a range of tools and functionalities specifically designed for labelling tasks. AI-users can leverage QGIS's labelling tools to create annotations such as points, lines, polygons, or complex geometries directly on the geospatial data. They can assign labels, attributes, or other relevant information to these annotations, facilitating the labelling process.</li> <li>Attribute Editing: QGIS allows AI-users to edit the attributes associated with geospatial features. This feature is particularly useful when labelling tasks involve updating or modifying existing attribute values. AI-users can easily access and edit attribute tables within QGIS, ensuring accurate and consistent labelling of geospatial data.</li> <li>Advanced Labelling Options: QGIS provides advanced labelling options to enhance the visual representation of labelled data. AI-users can configure labelling styles, such as font size, colour, and placement, to ensure readability and clarity of the labels. QGIS also supports labelling rules based on specific conditions or attribute values, allowing for automated labelling of geospatial features.</li> <li>Quality Control and Validation: QGIS offers tools for quality control and validation of labelled data. AI-users can use QGIS to review and verify the accuracy and consistency of labels by comparing them with reference data or applying predefined validation rules. This helps ensure the high quality of the labelled dataset.</li> <li>Integration with Existing Workflows: QGIS can be seamlessly integrated into AI-users's existing labelling workflows. They can import and export labelled data in various formats, making it compatible with other tools or systems. QGIS's flexibility allows AI-users to incorporate the labelled data generated within the remote desktop environment into her broader analysis pipelines or downstream applications.</li> </ul> <p>By leveraging QGIS in the remote desktop environment, AI-users gains access to a comprehensive labelling toolset specifically tailored for geospatial data. QGIS's geospatial data exploration capabilities, labelling tools, attribute editing features, advanced labelling options, quality control functionalities, and integration capabilities ensure efficient and accurate labelling tasks.</p>"},{"location":"3-plus/1-qgis/#starting-qgis","title":"Starting QGIS","text":"<p>The QGIS-dedicated platform can be launched from the JupyterHub dashboard login page. When asked which Server Option to launch, select \"QGIS (includes tooling and plugings v0.4 aws)\" and then <code>Start</code> to launch it. </p> <p></p> <p>QGIS can then be launched by opening a terminal, typing <code>qgis</code> and executing it. The QGIS window will be displayed.</p> <p></p> <p>More documentation on QGIS can be found on the official webpage (https://www.qgis.org/)[https://www.qgis.org/]. </p>"},{"location":"3-plus/2-stac/","title":"STAC API","text":""},{"location":"3-plus/2-stac/#introduction","title":"Introduction","text":"<p>The SpatioTemporal Asset Catalog (STAC) is a powerful standard for describing geospatial data, so it can be more easily worked with, indexed, discovered and shared. As part of the Application Hub, the system integrates the STAC Browser, which allows AI-users to discover and explore EO data and training datasets. The STAC Browser supports easy search and retrieval of relevant EO data through the STAC standard, enabling efficient data discovery and selection for ML tasks.</p> <p>The STAC Browser enhances AI-users's workflows by providing a user-friendly interface for browsing and exploring the available EO data and training datasets. The following highlights the key features and benefits of using the STAC Browser to support AI-users scenarios:</p> <ul> <li>User-Friendly Data Exploration: The STAC Browser provides AI-users with an intuitive and user-friendly interface to explore and discover available EO data and training datasets. Through the browser's interface, AI-users can navigate through different catalog layers, filter data based on various criteria such as location, time range, and sensor type, and preview metadata associated with each dataset. This streamlines the process of finding relevant data for her specific tasks.</li> <li>Metadata Visualization and Exploration: The STAC Browser allows AI-users to visualise and explore metadata associated with EO data and training datasets. They can view essential information such as acquisition dates, sensor specifications, data formats, and available assets within each dataset. The browser may also provide interactive visualisations, such as maps or charts, to help AI-users gain insights into the spatial and temporal characteristics of the data.</li> <li>Advanced Search and Filtering: The STAC Browser supports advanced search and filtering capabilities, enabling AI-users to narrow down her search based on specific criteria. They can apply filters based on geographic extent, temporal range, data source, and other relevant metadata attributes. This helps AI-users quickly locate the data and training datasets that align with her specific needs, saving time and effort in the data discovery process.</li> <li>Dataset Previews and Samples: The STAC Browser allows AI-users to preview sample images or data subsets from the available datasets. This preview functionality gives her a glimpse of the data content, allowing her to assess the suitability of the datasets for her tasks. It enables quick visual inspection to verify if the data aligns with her requirements before proceeding with further processing or integration into her workflows.</li> <li>Integration with STAC and STAC API: The STAC Browser seamlessly integrates with the STAC standard and STAC API, providing a unified experience for data discovery. It leverages the underlying STAC metadata and data catalog structure to present a comprehensive view of available EO data and training datasets. The browser may interact with the STAC API to fetch the relevant metadata and facilitate efficient data access and retrieval.</li> </ul> <p>By incorporating a STAC Browser into the service, AI-users gain a powerful tool for EO data and training datasets discovery. The browser's user-friendly interface, advanced search capabilities, and metadata visualisation features simplify the process of finding and exploring relevant data. This enables AI-users to efficiently identify and select the data necessary for her training datasets and other ML workflows.</p> <p>A dedicated article entitled Discovering Labelled EO Data with STAC describes comprehensively how to exploit the STAC format to discover labelled EO data.</p>"},{"location":"3-plus/2-stac/#starting-stac-browser","title":"Starting STAC Browser","text":"<p>The STAC Browser app can be launched at login with the option \"STAC Browser for AI-Extensions STAC API\".</p> <p></p> <p>After login, the STAC Browser dashboard will appear, showing the existing collections, which you can browse and visualise. </p> <p></p>"},{"location":"3-plus/2-stac/#accessing-via-stac-api-endpoint","title":"Accessing via STAC API endpoint","text":"<p>The dedicated STAC API endpoint can also be accessed via Jupyter Notebook by providing the appropriate authorisation <code>headers</code>. </p> <p><pre><code>payload = {\n    \"client_id\": \"ai-extensions\",\n    \"username\": \"ai-extensions-user\",\n    \"password\": os.environ.get(\"IAM_PASSWORD\"),\n    \"grant_type\": \"password\",\n}\n\ntoken = get_token(url=os.environ.get(\"IAM_URL\"), **payload)\ndel(payload)\nheaders = {\"Authorization\": f\"Bearer {token}\"}\n\ncat = Client.open(\"https://ai-extensions-stac.terradue.com\", headers=headers, ignore_conformance=True)\n</code></pre> To show the available collections in the Catalog. <pre><code>[c for c in cat.get_collections()]\n\n[&lt;CollectionClient id=ai-extensions-svv-dataset-labels&gt;,\n &lt;CollectionClient id=sentinel-s2-l2a-cogs&gt;,\n &lt;CollectionClient id=EUROSAT_2024_dataset&gt;,\n &lt;CollectionClient id=gisat-col&gt;]\n</code></pre></p>"},{"location":"4-scenarios/scenario1/","title":"User Scenario 1 - Alice does Exploratory Data Analysis (EDA)","text":"<p>The purpose of Explaratory Data Analysis (EDA) is to analyse the data that will be used to train and evaluate Machine Learning models. In this Notebook are firstly shown the steps to access and visualize EO data (e.g. Sentinel-2 scenes) and their metadata using STAC API. Secondly, a pre-arranged DataFrame containing labeled geospatial data with reflectance values and vegetation indices is loaded and used for the purpose of EDA.</p> <p>The requirements defined in the User Scenario 1 are:</p> <ul> <li>Import Libraries for data manipulation (e.g. <code>pandas</code>), visualisation (e.g. <code>matplotlib</code>, <code>seaborn</code>), geospatial analysis (e.g <code>rasterio</code>).</li> <li>Connect to STAC API for accessing Sentinel-2 data and their metadata, as well as DataFrames that contain labelled geospatial data, using appropriate authentication credentials.</li> <li>Query and retrieving the Data using specific parameters to filter out retrieved data (e.g time range, spatial extent, cloud cover).</li> <li>Visualising data (eg. S-2 metadata, spectral bands) and charts / graphs of the analysed relevant information (e.g. with <code>matplotlib</code>, <code>seaborn</code>). These are useful for gaining information about the data and for performing other tasks such as band combination, cloud masking, etc.</li> <li>Perform analysis on the EO data labels to gain insights and understand patterns. This includes tasks such as calculating statistical summaries, generating histograms or scatter plots, as well as making correlation matrix for understanding relationships between variables.</li> <li>Document and share of results and findings of the EDA process by exporting charts and visualisation plots into a report.</li> </ul> <p>The link to the Notebook for User Scenario 1 is: https://github.com/ai-extensions/notebooks/blob/main/scenario-1/s1-eda.ipynb.</p> <p>For more information and practical examples, see the related article Exploratory Data Analysis.</p>"},{"location":"4-scenarios/scenario10/","title":"User Scenario 10 - Eric discovers a model and consumes it","text":"<p>In this Scenario, a geospatial data analyst such as Eric discovers an ML model, created by other practitioners such as Alice, by using the STAC catalog\u2019s search functionalities. This enables Eric to use relevant keywords, such as the model name, geographic location, or application domain, to narrow down the search. The data catalog's search interface allows Eric to explore the available ML models that match the search criteria and are described using STAC. Once the ML model is found, it can be run on other geospatial data, integrating it into a larger workflow, or applying it within a specific application context. The ML model will then be deployed as a processing service on an exploitation platform using the OGC API processes. By following these steps, Eric can seamlessly integrate Alice's model into his existing infrastructure and leverage its capabilities within his geospatial analysis workflows.</p> <p>This Scenario has not been implemented yet, it's work in progress...</p>"},{"location":"4-scenarios/scenario2/","title":"User Scenario 2 - Alice labels Earth Observation data","text":"<p>Labelling data is a crucial step in the process for developing supervised Machine Learning (ML) models. It involves the critical task of assigning relevant labels or categories to different features within the data, such as land cover class (e.g. vegetation, water bodies, urban area, etc.) or other physical characteristics of the Earth's surface. These labels can be binary (e.g., water or non-water) or multi-class (e.g., forest, grassland, urban).</p> <p>The requirements defined in the User Scenario 2 are:</p> <ul> <li>Import libraries for data manipulation (e.g. <code>pandas</code>), visualisation (e.g. <code>matplotlib</code>,<code>seaborn</code>), geospatial analysis (e.g <code>rasterio</code>).</li> <li>Create labelling layers, using e.g. QGIS or an interactive map based on <code>Leafmap</code>, by creating new vector layers or annotations to mark the labelled areas or features, and export them in formats suitable for further analysis or machine learning tasks, such as shapefiles, GeoJSON, or raster formats.</li> <li>Connect to STAC API for accessing to Sentinel-2 data, using appropriate authentication credentials.</li> <li>Query and retrieving the Data using specific parameters to filter out retrieved data (e.g time range, spatial extent, cloud cover).</li> <li>Validate the labelled data to ensure its accuracy and reliability, comparing it to a reference dataset.</li> <li>Use Labelled Data for Supervised Machine Learning to train models, evaluate their performance, and make predictions on new, unlabeled EO data.</li> </ul> <p>The link to the Notebook for User Scenario 2 is: \u200b\u200bhttps://github.com/ai-extensions/notebooks/blob/main/scenario-2/s2-labellingEOdata.ipynb.</p> <p>For more information and practical examples, see the related article Labelling EO Data.</p>"},{"location":"4-scenarios/scenario3/","title":"User Scenario 3 - Alice describes the labelled Earth Observation data","text":"<p>Labeled Earth Observation (EO) data can be described using the SpatioTemporal Asset Catalog (STAC) standard. This allows to describe the labeled EO data while defining standardized sets of metadata to delineate its key properties, such as spatial and temporal extents, resolution, and other pertinent characteristics. Additionally, it enables the user to include details about the labeling process itself, as well as enabling specific parameters-driven queries on the catalog.</p> <p>The requirements defined in the User Scenario 3 are:</p> <ul> <li>Import libraries, including those for connecting to STAC API (e.g. <code>pystac</code>).</li> <li>Load labelled data (e.g. <code>.geojson</code> files).</li> <li>Create a STAC Item for each labelled EO data, including relevant metadata and STAC Extensions to describe the labelled data (e.g. the <code>stac-extensions/label extension</code> to provide detailed labelling information, or the <code>stac-extensions/asset</code> to associate assets or additional files with the labelled data).</li> <li>Extract metadata of the created STAC item, and show its relevant information, including visualisation on spatial map.</li> </ul> <p>The link to the Notebook for User Scenario 3 is: \u200b\u200bhttps://github.com/ai-extensions/notebooks/blob/main/scenario-3/s3-describingEOdata.ipynb.</p> <p>For more information and practical examples, see the related article Describing Labelled EO Data with STAC.</p>"},{"location":"4-scenarios/scenario4/","title":"User Scenario 4 - Alice discovers labelled Earth Observation data","text":"<p>This Scenario documents the process for discovering labelled EO data described with the STAC standard. One of the notable advantages of the STAC-supported catalogs is the ability to filter search results using STAC metadata. This empowers the user to narrow down the search based on specific labeling criteria. By applying such filters, the user can pinpoint the datasets that align with the specific requirements.</p> <p>This Scenario explores two ways to discover, query and access labeled EO datasets: by using the STAC Browser, which provides a user-friendly graphical interface, or by using the STAC API endpoint, which enables the user to access to the STAC collection programmatically. </p> <p>The requirements defined in the User Scenario 4 are:</p> <ul> <li>Import Libraries (e.g. <code>pystac</code>).</li> <li>Understand about the STAC standard.</li> <li>Interact with a STAC Browser tool or the STAC API endpoint to access STAC data, by defining the search criteria based on the requirements (e.g. time range, geographic extent, etc.).</li> <li>Examine the search results to explore the available labeled EO datasets (view metadata summaries) and identify a dataset that meets the requirements to access associated assets.</li> <li>Depending on the access permissions and licensing, the user identifies labelled EO a dataset stored on AWS S3 bucket and is given the option to either download it or integrate it into the workflow for further analysis or model training.</li> </ul> <p>The link to the Notebook for User Scenario 4 is: \u200b\u200bhttps://github.com/ai-extensions/notebooks/blob/main/scenario-4/s4-discoveringLabelledEOData.ipynb.</p> <p>For more information and practical examples, see the related article Discovering Labelled EO Data with STAC.</p>"},{"location":"4-scenarios/scenario5/","title":"User Scenario 5 - Alice develops a new Machine Learning model","text":"<p>During the implementation of this Scenario, two Notebooks were developed:</p> <ul> <li>\u201cAlice develops a new machine learning\u201d Notebook: this is the main Notebook for Scenario 5, as it follows the requirements for Scenario 5 described in the Service Specification document. </li> <li>\u201cImplementation of EuroSAT STAC dataset\u201d Notebook: the objective of this Notebook was to generate STAC Catalog, Collection and Items of the EuroSAT dataset. The EuroSAT STAC dataset was then used as input dataset for developing and testing the main \u201cAlice develops a new machine learning\u201d Notebook. </li> </ul>"},{"location":"4-scenarios/scenario5/#notebook-for-alice-develops-a-new-machine-learning","title":"Notebook for \u201cAlice develops a new machine learning\u201d","text":"<p>The AI-user employs MLflow Tracking as a crucial tool throughout the ML model development cycle. It ensures effective log tracking and preserves vital information, including specific code versions, datasets used, and model hyperparameters. By logging this information, the reproducibility of the work drastically increases, enabling users to revisit and replicate past experiments accurately. Moreover, quality metrics have been tracked during the task, such as classification accuracy, loss function fluctuations, and inference time, enabling easy comparison between different models.</p> <p>A pivotal component of the platform is the Model Registry, which is seamlessly integrated with the experiment tracking functionality. The Model Registry acts as a central repository, allowing the user to manage and oversee their models effectively. This preservation of trained models is of significant economic importance, as training a model often requires substantial time and computing resources.</p> <p>The requirements defined in the User Scenario 5 that were included in this Notebook are:</p> <ul> <li>Import Libraries (e.g. <code>tensorflow</code>, <code>mlflow</code>, <code>sklearn</code>)</li> <li>Clarify the specific problem, identify the input data, and provide a pipeline for data ingestion. This ensures training a high-resolution classifier.</li> <li>Design the model architecture with the advantage of Convolutional Neural Networks (CNNs). The architecture may encompass different hidden layers with a variety of activation functions (such as ReLU, Sigmoid, tanh) and a couple of techniques that enhance the stability and reliability of the model, such as Batch Normalization and Regularization (namely Dropout).</li> <li>Train the model using the EuroSAT STAC training dataset and evaluate the performance of the trained model on the test dataset. </li> <li>Evaluate the ML model with evaluation metrics such as accuracy, precision, recall, F1 score, and the confusion matrix, in order to determine the model's effectiveness.</li> <li>Fine-tune the ML model by adjusting hyperparameters, modifying the architecture, or incorporating regularization techniques, as needed. This step may be repeated several times to reach the best performance and log the impact of each adjustment on the performance of the output model. </li> <li>Use MLflow Tracking tools (e.g. autolog functionality) to log relevant information such as hyperparameters, model configurations, training metrics, model weights, and evaluation results using MLflow APIs. </li> </ul> <p>The link to this Notebook is: \u200b\u200bhttps://github.com/ai-extensions/notebooks/blob/main/scenario-5/trials/s5-newMLModel.ipynb.</p> <p>For more information and practical examples, see the related article Developing a new ML model and tracking with MLflow. </p>"},{"location":"4-scenarios/scenario5/#notebook-for-implementation-of-eurosat-dataset-stac","title":"Notebook for \u201cImplementation of EuroSAT dataset STAC\u201d","text":"<p>The EuroSAT dataset is based on ESA's Sentinel-2 data, covering 13 spectral bands and consisting out of 10 classes with a total of 27,000 labeled and geo-referenced images (https://github.com/phelber/EuroSAT). This Notebook was used to create a STAC Catalog, a STAC Collection, and STAC Items for the entire EuroSAT dataset. Each STAC Item was generated with three assets: the EuroSAT's georeferenced patches (in .tif format), their labels (in <code>.geojson</code> format), and their RGB-composite thumbnails (in <code>.jpeg</code> format). These STAC objects were posted into ESA-AI dedicated S3 bucket (s3://ai-ext-bucket-dev/), and subsequently posted on the ESA-AI dedicated STAC endpoint (https://ai-extensions-stac.terradue.com). </p> <p>The requirements that were included in this Notebook are:</p> <ul> <li>Import Libraries (e.g., <code>pystac</code>, <code>torchgeo</code>).</li> <li>Load the EuroSAT/EuroSAT100 Dataset using the torchgeo API. </li> <li>Generate STAC objects (STAC catalog, STAC collection, and STAC Item) with their corresponding characteristics, namely, id, properties, assets, STAC extensions, etc., using pystac standard conventions and place them in a .json file. The class label for each patch of data is stored in a <code>.geojson</code> file format.</li> <li>Post the generated STAC objects into the S3 bucket s3://ai-ext-bucket-dev.</li> <li>Publish all STAC items into the dedicated collection \u201cEUROSAT_2024_dataset\u201d in the STAC endpoint (https://ai-extensions-stac.terradue.com), and then validate by checking the published items.</li> </ul> <p>The link to the Notebook for User Scenario 5 is: https://github.com/ai-extensions/notebooks/blob/main/scenario-5-stac-dataloader/s5-stac-dataloder.ipynb. </p>"},{"location":"4-scenarios/scenario6/","title":"User Scenario 6 - Alice starts a training job on a remote machine","text":"<p>In this scenario, the ML practitioner Alice develops two Earth Observation (EO) Application Packages using the Common Workflow Language (CWL), as described in the OGC proposed best practices. The two App Packages CWLs are executed on a remote machine with a CWL runner for Kubernetes, enabling the submission of (parallel) kubernetes jobs distributed across the available resources in the cluster. MLflow is used for tracking experiments and related metrics for further analysis and comparison.</p> <ul> <li>App Package CWL for a training job: Alice develops an ML training job with <code>RandomForest</code>, based on a segmentation approach for water bodies masking. The training process is repeated and evaluated for each training run with MLflow and in the end, the model evaluated with higher performance is selected and used for the inference service.</li> <li>App Package CWL for inference job: this service performs inference on Sentinel-2 data, based on the best model developed with the training service. It takes as input parameter the STAC Item(s) of Sentinel-2 data and generates the inference water-bodies output mask(s).</li> </ul> <p>The requirements defined in the User Scenario 6 are:</p> <ul> <li>Write a Python application with its containerised environment</li> <li>Use the CWL standard, as described in the OGC proposed best practices, for writing EO Application Packages</li> <li>Dockerise all processing nodes and write the App Package CWLs through CI/CD pipeline with GitHub Actions, and release them as official repository releases</li> <li>Use the CWL runners for Kubernetes to submit (parallel) jobs, distributing them across the available resources in the cluster</li> <li>Configure MLflow server for tracking the experiments and log relevant information</li> </ul> <p>The links to the App Package CWL files are: </p> <ul> <li>App Package CWL for training job: https://github.com/ai-extensions/notebooks/releases/download/v1.0.8/water-bodies-app-training.1.0.8.cwl </li> <li>App Package CWL for inference job: https://github.com/ai-extensions/notebooks/releases/download/v1.0.8/water-bodies-app-inference.1.0.8.cwl </li> </ul> <p>For more information and practical examples, see the related article Training and Inference on a remote machine. </p>"},{"location":"4-scenarios/scenario7/","title":"User Scenario 7 \u200b- Alice describes her trained machine learning model","text":"<p>This Scenario leverabes the capabilities of STAC to provide a comprehensive and standardised description of a trained ML model. This is done with a STAC Item file that encapsulates the relevant metadata, such as: model name and version, description of the model architecture and training process, specifications of input and output data formats, performance metrics and evaluation results, details regarding model deployment and usage. This approach not only facilitates collaboration but also promotes interoperability within the geospatial and ML communities. extensions</p> <p>The requirements defined in the User Scenario 7 are:</p> <ul> <li>Import Libraries (e.g. <code>pystac</code>, <code>boto3</code>).</li> <li>Option to either create a STAC Item with <code>pystac</code>, or to upload an existing STAC Item into the Notebook. The STAC Item will contain all related ML model specific properties, related STAC extensions and hyperparameters.</li> <li>Create interlinked STAC Item, Catalog and Collection, and the STAC folder structure.</li> <li>Post STAC files onto S3 bucket.</li> <li>Publish STAC files onto STAC endpoint.</li> <li>Search STAC items on STAC endpoint with standard query params such as bbox and time range, but also with ML-specific params such as model architecture, hyperparameters.</li> </ul> <p>The link to the Notebook for User Scenario 7 is: https://github.com/ai-extensions/notebooks/blob/main/scenario-7/s7-CreateSTAC-describingMLmodel.ipynb.</p> <p>For more information and practical examples, see the related article Describig a trained ML model with STAC. </p>"},{"location":"4-scenarios/scenario8/","title":"User Scenario 8 -\u200b Alice reuses an existing pre-trained model","text":"<p>This Scenario demonstrates the process of reusing a pre-trained model by leveraging the power of transfer learning. Transfer learning, a widely adopted technique in deep learning, involves using an existing model, pre-trained on a large dataset (such as ImageNet), to train a new model on a smaller, task-specific dataset. This approach allows the new model to effectively utilize the features learned by the pre-trained model, enabling it to extract valuable information from the input data more efficiently. Consequently, the new model can achieve higher accuracy even with limited data.</p> <p>This implementation was performed for a semantic segmentation task in the scope of Earth Observation (EO). The process involves searching and downloading a suitable EO dataset already annotated by experts and that includes both EO data (in this case Sentinel-2 data) and their corresponding masks. After performing the appropriate feature engineering on the data, the user fine-tuned the pre-trained model to provide high-resolution results. This step could leverage the GPU resources set-up in the dedicaetd App Hub environment (with the Machine Learning Lab with GPU 0.10 profile), significantly reducing the execution time for fine-tuning. Finally, the model was assessed and tested on unseen data.</p> <p>The requirements defined in the User Scenario 8 are:</p> <ul> <li>Import libraries (e.g. <code>torch</code>, <code>sklearn</code>, <code>albumentation</code>).</li> <li>Data acquisition, including EO data search and data loader implementing different augmentation techniques (e.g., <code>RandomCrop</code>, <code>Resize</code>, and <code>RandomRotate90</code>) and data loading in batches.</li> <li>Data visualization to enable the user to gain comprehensive insights of data distribution.</li> <li>Selection of pre-trained ML model, in this case with a UNet backbone trained on ImageNet, and subsequently implementation of fine-tuning adjustments.</li> <li>Evaluation of outputs using different techniques on unseen dataset (e.g., plotting Loss functions, calculating mIOU).</li> <li>Inference on the unseen test dataset.</li> </ul> <p>The link to the Notebook for User Scenario 8 is: https://github.com/ai-extensions/notebooks/blob/main/scenario-8/s8_Transfer_Learning.ipynb.</p> <p>For more information and practical examples, see the related article Reusing an existing pre-trained ML model. </p>"},{"location":"4-scenarios/scenario9/","title":"User Scenario 9 - Alice creates a training dataset","text":"<p>This scenario demonstrates how to create a deep learning dataset using image chips. Annotating a dataset is a meticulous and time-consuming process that demands precision and focus. To ensure accuracy, the dataset must be reviewed by multiple experts. However, various computer vision techniques can expedite this process. For instance, an initial labeling can be done by another ML model, with human experts refining these labels to ensure their correctness.</p> <p>In this Scenario, the user generates a labelled dataset for a semantic segmentation task using:</p> <ul> <li>a human-annotation approach with the IRIS Tool, an AI-assisted tool that enhances image segmentation and classification for satellite imagery and other types of images, developed by ESA/ESRIN Phi-Lab (see documentation).</li> <li>an automated ML-driven approach consisting on a <code>RandomForest</code> model for segmenting task. </li> </ul> <p>The process of creating the training dataset is finalised with the creation of the STAC objects of each data (i.e. the generated image patches and their corresponding masks), and the related publication on a dedicated S3 bucket and STAC endpoint.</p> <p>The requirements defined in the User Scenario 9 are:</p> <ul> <li>Import libraries (e.g., <code>pystac</code>, <code>rasterio</code>, <code>boto3</code>, <code>sklearn</code>).</li> <li>Load the Sentinel-2 data using STAC.</li> <li>Generate EO image patches with their corresponding masks (annotated with three classes <code>water</code>, <code>non-water</code> and <code>not-applicable</code>), using both an AI-driven approach and an human-annotation approach.</li> <li>Create STAC Objects, including STAC Item for each image patch, STAC collection, and STAC catalog to describe the dataset with its metadata.</li> <li>Post the STAC Objects to a dedicated S3 bucket.</li> <li>Publish the STAC Objects into a dedicated STAC endpoint.</li> </ul> <p>The link to the Notebook for User Scenario 9 is: https://github.com/ai-extensions/notebooks/blob/main/scenario-9/s9-CreatingTrainingData.ipynb.</p> <p>For more information and practical examples, see the related article Creating a training Dataset.</p>"}]}